package lessons.week9;

/**
 * @version 1.0
 * @Description: 海量数据处理
 * @author: bingyu
 * @date: 2023/3/1
 */
public class MassiveDataProcessing {


    /*
     一、海量数据处理面对两个问题:
        1.一台机器的内存存不下
        2.一台机器处理起来太慢

     二、海量数据处理的核心思想: 分治
        单机: 利用外存，分批加入内存处理
        多机: 对数据分片，利用多机内存存储
        多机: 并行计算，利用多线程、多机并行处理

     三、一些处理技巧
        1.外部排序:多路归并、桶排序
        2.哈希分片
        3.位图

     四、常见的问题：
        1.海量数据排序
        2.海量数据查询
        3.海量数据TOP K
        4.海量数据求频率TOP K
        5.海量数据去重/找重
        6.两个海量数据文件找重

     五、具体题目。
       1.按照金额大小给10GB的订单文件进行排序(假设内存不够10GB)
         TODO: 类似归并排序的处理思路,先将10GB的文件依次取1GB数据到内存里进行排序，生产文件放入外存，得到 0.txt、1.txt、2.txt.......9.txt
                这10个排好序的1GB文件，然后我们申请长度为10的数组，我们每次从每个文件里面顺序的读取一个数据出来放到数组里面，然后从中取出最小的数据
                min放入结果文件result.txt里，假设该min数据来自1.txt文件，那么我们需要继续顺序读取1.txt文件里的数据，来顶替之前min空出来的位置,
                再取数组中的最小值放入结果文件result.txt里，依次循环执行直到所有文件的处境全部处理完，那么得到的结果文件result.txt就是最终排好序
                的10GB文件了!


       2.有一个ip地址白名单文件，包含10亿个IP地址，判断某IP是否在白名单中?


       3.10亿个整数，判断某个整数是否在其中?

       4.10亿个整数，放在文件中，内存有限，如何求最大的TOP100个整数?

       5.100GB的搜索关键字文件，统计出现频率TOP前100的关键词

       6.一个文件中包含10亿条URL，有可能会重复，将重复的去掉

       7.给你a、b两个文件，各自有50亿条URL，每条URL占用64字节，内存限制是4GB，找出a、b文件共同的URL

    */
}
